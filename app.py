from flask import Flask, jsonify, request, send_from_directory
from dateutil import parser as date_parser
from datetime import datetime, timedelta
import json
import os
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import requests
import random
import re
from concurrent.futures import ThreadPoolExecutor
import time
import subprocess
from threading import Thread
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage
from email.utils import formataddr
from PIL import Image
import io
sent_demo_alerts = set()

ALERTS_FILE = 'alerts.json'
NOTIFIED_FILE = 'notified.json'
MASTER_JSON_URL = "https://js9467.github.io/Brtourney/settings.json"
DEMO_EVENTS_FILE = 'demo_events.json'
DEMO_LEADERBOARD_FILE = 'demo_leaderboard.json'

SMTP_USER = "bigrockapp@gmail.com"
SMTP_PASS = "coslxivgfqohjvto"  # Gmail App Password
SMTP_SERVER = "smtp.gmail.com"
SMTP_PORT = 587




app = Flask(__name__)
CACHE_FILE = 'cache.json'
EVENTS_FILE = 'events.json'
SETTINGS_FILE = 'settings.json'
DEMO_DATA_FILE = 'demo_data.json'



MASTER_JSON_URL = "https://js9467.github.io/Brtourney/settings.json"
API_KEY = "e6f354c9c073ceba04c0fe82e4243ebd"

def fetch_html(url):
    """Fetch page HTML using ScraperAPI to bypass SSL issues."""
    full_url = f"http://api.scraperapi.com?api_key={API_KEY}&url={url}"
    print(f"üì° Fetching via ScraperAPI: {url}")
    resp = requests.get(full_url, timeout=60)
    if resp.status_code == 200:
        return resp.text
    print(f"‚ö†Ô∏è Failed to fetch HTML: {resp.status_code}")
    return ""

def load_alerts():
    if os.path.exists(ALERTS_FILE):
        with open(ALERTS_FILE) as f:
            return json.load(f)
    return []

def save_alerts(alerts):
    with open(ALERTS_FILE, 'w') as f:
        json.dump(alerts, f, indent=2)

def load_notified_events():
    if os.path.exists(NOTIFIED_FILE):
        with open(NOTIFIED_FILE) as f:
            return set(json.load(f))
    return set()


def save_notified_events(notified):
    with open(NOTIFIED_FILE, 'w') as f:
        json.dump(list(notified), f)

def send_sms_email(phone_email, message):
    msg = MIMEText(message)
    msg['From'] = SMTP_USER
    msg['To'] = phone_email
    msg['Subject'] = "Boat Alert"

    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
        server.starttls()
        server.login(SMTP_USER, SMTP_PASS)
        server.sendmail(SMTP_USER, [phone_email], msg.as_string())

def send_boat_email_alert(event):
    boat = event.get('boat', 'Unknown')
    action = event.get('event', 'Activity')
    timestamp = event.get('timestamp', datetime.now().isoformat())
    uid = event.get('uid', 'unknown')
    details = event.get('details', 'No additional details provided')

    # ‚úÖ Subject line includes details if available
    subject = f"{boat} {action}"
    if details and details.lower() != 'hooked up!':
        subject += f" ‚Äî {details}"
    subject += f" at {timestamp}"

    # üîπ Detect the real image file
    base_path = f"static/images/boats/{uid}"
    image_path = None
    for ext in [".jpg", ".jpeg", ".png", ".webp"]:
        candidate = base_path + ext
        if os.path.exists(candidate):
            image_path = candidate
            break

    # üîπ Fallback to Palmer Lou if missing
    if not image_path and os.path.exists("static/images/palmer_lou.jpg"):
        print(f"‚ö†Ô∏è No image for {boat}, using fallback Palmer Lou")
        image_path = "static/images/palmer_lou.jpg"

    recipients = load_alerts()
    if not recipients:
        return 0

    success = 0

    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASS)

            for recipient in recipients:
                try:
                    msg = MIMEMultipart("related")
                    msg['From'] = formataddr(("BigRock Alerts", SMTP_USER))
                    msg['To'] = recipient
                    msg['Subject'] = subject  # ‚úÖ Now includes details

                    msg_alt = MIMEMultipart("alternative")
                    msg.attach(msg_alt)

                    # ‚úÖ Plain text version
                    text_body = f"""üö§ {boat} {action}!
Time: {timestamp}
Details: {details}

BigRock Live Alert
"""
                    msg_alt.attach(MIMEText(text_body, "plain"))

                    # ‚úÖ HTML version
                    html_body = f"""
                    <html><body>
                        <p>üö§ <b>{boat}</b> {action}!<br>
                        Time: {timestamp}<br>
                        Details: {details}</p>
                        <img src="cid:boat_image" style="max-width: 600px; height: auto;">
                    </body></html>
                    """
                    msg_alt.attach(MIMEText(html_body, "html"))

                    # üîπ Attach image if available
                    if image_path and os.path.exists(image_path):
                        with Image.open(image_path) as img:
                            if img.mode in ("RGBA", "LA"):
                                img = img.convert("RGB")
                            img.thumbnail((600, 600))
                            img_bytes = io.BytesIO()
                            img.save(img_bytes, format="JPEG", quality=70)
                            img_bytes.seek(0)
                            image = MIMEImage(img_bytes.read(), name=f"{uid}.jpg")
                            image.add_header("Content-ID", "<boat_image>")
                            image.add_header("Content-Disposition", "inline", filename=f"{uid}.jpg")
                            msg.attach(image)

                    server.sendmail(SMTP_USER, [recipient], msg.as_string())
                    print(f"‚úÖ Email alert sent to {recipient} for {boat} {action}")
                    success += 1

                except Exception as e:
                    print(f"‚ùå Failed to send alert to {recipient}: {e}")

    except Exception as e:
        print(f"‚ùå SMTP batch failed: {e}")

    return success



def fetch_with_scraperapi(url):
    api_key = "e6f354c9c073ceba04c0fe82e4243ebd"
    full_url = f"http://api.scraperapi.com?api_key={api_key}&url={url}"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    try:
        response = requests.get(full_url, headers=headers, timeout=30)
        if response.status_code == 200:
            return response.text
        else:
            print(f"‚ö†Ô∏è ScraperAPI failed: HTTP {response.status_code}")
    except Exception as e:
        print(f"‚ùå Error fetching via ScraperAPI: {e}")
    return ""

def get_cache_path(tournament, filename):
    folder = os.path.join("cache", normalize_boat_name(tournament))
    os.makedirs(folder, exist_ok=True)
    return os.path.join(folder, filename)


def load_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r') as f:
            return json.load(f)
    return {}

def save_cache(cache):
    with open(CACHE_FILE, 'w') as f:
        json.dump(cache, f, indent=2)

def load_settings():
    if os.path.exists(SETTINGS_FILE):
        with open(SETTINGS_FILE, 'r') as f:
            return json.load(f)
    return {}

def load_demo_events():
    """Load demo mode events only."""
    if os.path.exists(DEMO_EVENTS_FILE):
        try:
            with open(DEMO_EVENTS_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading demo events: {e}")
    return []

def load_demo_leaderboard():
    """Load demo mode leaderboard only."""
    if os.path.exists(DEMO_LEADERBOARD_FILE):
        try:
            with open(DEMO_LEADERBOARD_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading demo leaderboard: {e}")
    return []


def get_data_source():
    settings = load_settings()
    return settings.get("mode", "live")

def is_cache_fresh(cache, key, max_age_minutes):
    try:
        last_scraped = cache.get(key, {}).get("last_scraped")
        if not last_scraped:
            return False
        last_time = datetime.fromisoformat(last_scraped)
        return (datetime.now() - last_time) < timedelta(minutes=max_age_minutes)
    except Exception:
        return False

def get_current_tournament():
    try:
        with open(SETTINGS_FILE, 'r') as f:
            settings = json.load(f)
            return settings.get('tournament', 'Big Rock')
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load settings: {e}")
        return 'Big Rock'

def normalize_boat_name(name):
    if not name:
        return "unknown"
    return name.lower().replace(' ', '_').replace("'", "").replace("/", "_")

from threading import Lock

# Shared lock dictionary for image files
image_locks = {}

def cache_boat_image(boat_name, image_url):
    folder = BOAT_FOLDER
    os.makedirs(folder, exist_ok=True)
    safe_name = normalize_boat_name(boat_name)

    # Extract original file extension
    ext = os.path.splitext(image_url.split('?')[0])[-1].lower()
    if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
        ext = '.jpg'

    file_path = os.path.join(folder, f"{safe_name}{ext}")

    lock = image_locks.setdefault(file_path, Lock())
    with lock:
        # ‚úÖ If WebP version already exists, return it
        webp_path = os.path.join(folder, f"{safe_name}.webp")
        if os.path.exists(webp_path) and os.path.getsize(webp_path) > 0:
            return f"/{webp_path}"

        # ‚úÖ If original exists and WebP exists, return WebP
        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
            optimize_boat_image(file_path)
            if os.path.exists(webp_path):
                return f"/{webp_path}"
            return f"/{file_path}"

        # üîπ Download new image
        try:
            response = requests.get(image_url, timeout=10)
            if response.status_code == 200:
                with open(file_path, 'wb') as f:
                    f.write(response.content)
                print(f"‚úÖ Downloaded image for {boat_name}: {file_path}")

                # üîπ Optimize + Create WebP
                optimize_boat_image(file_path)

                # Prefer WebP if created
                if os.path.exists(webp_path):
                    return f"/{webp_path}"
                return f"/{file_path}"

            else:
                print(f"‚ö†Ô∏è Failed to download image for {boat_name}: HTTP {response.status_code}")
                return "/static/images/boats/default.jpg"

        except Exception as e:
            print(f"‚ö†Ô∏è Error downloading image for {boat_name}: {e}")
            if os.path.exists(file_path):
                os.remove(file_path)
            return "/static/images/boats/default.jpg"




def inject_hooked_up_events(events, tournament=None):
    """
    Create synthetic Hooked Up events 5‚Äì120 min before resolution events.
    Does NOT alter the original resolution timestamps.
    """
    demo_events = []
    inserted_keys = set()
    today = datetime.now().date()

    # Sort by actual timestamp
    events.sort(key=lambda e: date_parser.parse(e["timestamp"]))

    for event in events:
        boat = event.get("boat", "Unknown")
        uid = event.get("uid", "unknown")
        etype = event.get("event", "").lower()
        details = event.get("details", "").lower()

        # Only inject for resolution events
        is_resolution = (
            "boated" in etype
            or "released" in etype
            or "pulled hook" in details
            or "wrong species" in details
        )
        if not is_resolution:
            continue

        try:
            # Keep resolution as-is (optional: shift date to today)
            orig_dt = date_parser.parse(event["timestamp"])
            res_dt = datetime.combine(today, orig_dt.time())
            event["timestamp"] = res_dt.isoformat()  # ‚úÖ Demo uses today's date, same time

            # Create Hooked Up event 5‚Äì120 min before
            delta_minutes = random.randint(5, 120)
            hookup_dt = res_dt - timedelta(minutes=delta_minutes)
            if hookup_dt.date() < today:
                hookup_dt = datetime.combine(today, datetime.min.time()) + timedelta(minutes=1)

            key = f"{uid}_{res_dt.isoformat()}"
            if key in inserted_keys:
                continue

            demo_events.append({
                "timestamp": hookup_dt.isoformat(),
                "event": "Hooked Up",
                "boat": boat,
                "uid": uid,
                "details": "Hooked up!",
                "hookup_id": key
            })
            inserted_keys.add(key)

        except Exception as e:
            print(f"‚ö†Ô∏è Demo injection failed for {boat}: {e}")

    # Merge synthetic Hooked Up + original events, sort chronologically
    all_events = sorted(events + demo_events, key=lambda e: date_parser.parse(e["timestamp"]))
    print(f"üì¶ Returning {len(all_events)} events (with {len(demo_events)} hooked up injections)")
    return all_events



def save_demo_data_if_needed(settings, old_settings):
    """Save demo events and leaderboard into separate JSON files if in demo mode."""
    if settings.get("data_source") == "demo":
        print("üì¶ [DEMO] Saving demo data...")
        tournament = settings.get("tournament", "Big Rock")
        try:
            # 1Ô∏è‚É£ Scrape fresh events & leaderboard
            events = scrape_events(force=True, tournament=tournament)
            leaderboard = scrape_leaderboard(tournament)

            # 2Ô∏è‚É£ Inject Hooked Up demo events
            injected = inject_hooked_up_events(events, tournament)

            # 3Ô∏è‚É£ Save to separate files
            with open(DEMO_EVENTS_FILE, 'w') as f:
                json.dump(injected, f, indent=2)
            with open(DEMO_LEADERBOARD_FILE, 'w') as f:
                json.dump(leaderboard, f, indent=2)

            print(f"‚úÖ [DEMO] Demo files updated: {len(injected)} events, {len(leaderboard)} leaderboard entries")
        except Exception as e:
            print(f"‚ùå [DEMO] Failed to cache demo data: {e}")

def run_in_thread(target, name):
    def wrapper():
        try:
            print(f"üßµ Starting {name} scrape in thread...")
            target()
            print(f"‚úÖ Finished {name} scrape.")
        except Exception as e:
            print(f"‚ùå Error in {name} thread: {e}")
    Thread(target=wrapper).start()

def scrape_participants(force=False):
    cache = load_cache()
    tournament = get_current_tournament()
    participants_file = get_cache_path(tournament, "participants.json")
    os.makedirs(os.path.dirname(participants_file), exist_ok=True)

    # ‚úÖ Step 1: Respect cache freshness (1 day)
    if not force and is_cache_fresh(cache, f"{tournament}_participants", 1440):
        print("‚úÖ Participant cache is fresh ‚Äî skipping scrape.")
        if os.path.exists(participants_file):
            with open(participants_file, "r") as f:
                return json.load(f)
        return []

    try:
        # ‚úÖ Step 2: Load tournament settings
        settings_url = "https://js9467.github.io/Brtourney/settings.json"
        settings = requests.get(settings_url, timeout=30).json()
        matching_key = next((k for k in settings if k.lower() == tournament.lower()), None)
        if not matching_key:
            raise Exception(f"Tournament '{tournament}' not found in settings.json")

        participants_url = settings[matching_key].get("participants")
        if not participants_url:
            raise Exception(f"No participants URL found for {matching_key}")

        print(f"üì° Scraping participants from: {participants_url}")

        # ‚úÖ Step 3: Fetch HTML
        html = fetch_with_scraperapi(participants_url)
        if not html:
            raise Exception("No HTML returned from ScraperAPI")

        with open("debug_participants.html", "w", encoding="utf-8") as f:
            f.write(html)

        soup = BeautifulSoup(html, 'html.parser')
        updated_participants = {}
        seen_boats = set()
        download_tasks = []

        # ‚úÖ Step 4: Parse all boats
        for article in soup.select("article.post.format-image"):
            name_tag = article.select_one("h2.post-title")
            type_tag = article.select_one("ul.post-meta li")
            img_tag = article.select_one("img")

            if not name_tag:
                continue

            boat_name = name_tag.get_text(strip=True)
            if ',' in boat_name or boat_name.lower() in seen_boats:
                continue

            boat_type = type_tag.get_text(strip=True) if type_tag else ""
            uid = normalize_boat_name(boat_name)
            seen_boats.add(boat_name.lower())

            image_url = img_tag['src'] if img_tag and 'src' in img_tag.attrs else None

            # Initially, no image_path ‚Äî UI can load immediately with placeholders
            updated_participants[uid] = {
                "uid": uid,
                "boat": boat_name,
                "type": boat_type,
                "image_path": ""  # Will fill after download
            }

            if image_url:
                download_tasks.append((uid, boat_name, image_url))

        # ‚úÖ Step 5: Save JSON immediately so UI can load rapidly
        updated_list = list(updated_participants.values())
        with open(participants_file, "w") as f:
            json.dump(updated_list, f, indent=2)
        print(f"üíæ Initial participants.json written with {len(updated_list)} entries (images pending)")

        # ‚úÖ Step 6: Start background image downloads
        if download_tasks:
            print(f"üì∏ Downloading {len(download_tasks)} boat images in background...")

            def download_and_update(uid, boat_name, url):
                try:
                    result_path = cache_boat_image(boat_name, url)
                    updated_participants[uid]["image_path"] = result_path
                    # Update JSON incrementally for UI refresh
                    with open(participants_file, "w") as f:
                        json.dump(list(updated_participants.values()), f, indent=2)
                except Exception as e:
                    print(f"‚ùå Error downloading image for {uid}: {e}")
                    updated_participants[uid]["image_path"] = "/static/images/boats/default.jpg"

            with ThreadPoolExecutor(max_workers=6) as executor:
                for uid, bname, url in download_tasks:
                    executor.submit(download_and_update, uid, bname, url)

        # ‚úÖ Step 7: Update cache timestamp
        cache[f"{tournament}_participants"] = {"last_scraped": datetime.now().isoformat()}
        save_cache(cache)

        return updated_list

    except Exception as e:
        print(f"‚ö†Ô∏è Error scraping participants: {e}")
        return []


def scrape_events(force=False, tournament=None):
    cache = load_cache()
    settings = load_settings()
    tournament = tournament or get_current_tournament()
    events_file = get_cache_path(tournament, "events.json")

    # ‚úÖ Check cache freshness (2 minutes)
    cache_key = f"events_{tournament}"
    if not force and is_cache_fresh(cache, cache_key, 2):
        if os.path.exists(events_file):
            with open(events_file) as f:
                return json.load(f)
        return []

    try:
        # Load tournament settings
        settings_url = "https://js9467.github.io/Brtourney/settings.json"
        remote = requests.get(settings_url, timeout=15).json()
        key = next(
            (k for k in remote if normalize_boat_name(k) == normalize_boat_name(tournament)),
            None
        )
        if not key:
            raise Exception(f"Tournament '{tournament}' not found in remote settings.json")

        events_url = remote[key].get("events")
        if not events_url:
            raise Exception(f"No events URL found for {tournament}")

        print(f"üì° Scraping events from: {events_url}")
        html = fetch_with_scraperapi(events_url)
        if not html:
            raise Exception("Failed to fetch events page HTML")

        soup = BeautifulSoup(html, 'html.parser')
        events = []
        seen = set()

        # Load participants for boat name normalization
        participants_file = get_cache_path(tournament, "participants.json")
        participants = {}
        if os.path.exists(participants_file):
            with open(participants_file) as f:
                participants = {p["uid"]: p for p in json.load(f)}

        # Parse all event-like elements
        for article in soup.select("article.m-b-20, article.entry, div.activity, li.event, div.feed-item"):
            time_tag = article.select_one("p.pull-right")
            name_tag = article.select_one("h4.montserrat")
            desc_tag = article.select_one("p > strong")
            if not time_tag or not name_tag or not desc_tag:
                continue

            # Parse timestamp
            raw = time_tag.get_text(strip=True).replace("@", "").strip()
            try:
                ts = date_parser.parse(raw).replace(year=datetime.now().year).isoformat()
            except:
                continue

            # Normalize boat and event
            boat = name_tag.get_text(strip=True)
            desc = desc_tag.get_text(strip=True)
            uid = normalize_boat_name(boat)

            # Use official participant boat name if available
            if uid in participants:
                boat = participants[uid]["boat"]

            if "released" in desc.lower():
                event_type = "Released"
            elif "boated" in desc.lower():
                event_type = "Boated"
            elif "pulled hook" in desc.lower():
                event_type = "Pulled Hook"
            elif "wrong species" in desc.lower():
                event_type = "Wrong Species"
            else:
                event_type = "Other"

            # Deduplicate
            key = f"{uid}_{event_type}_{ts}"
            if key in seen:
                continue
            seen.add(key)

            events.append({
                "timestamp": ts,
                "event": event_type,
                "boat": boat,
                "uid": uid,
                "details": desc
            })

        # Sort and save
        events.sort(key=lambda e: e["timestamp"])
        with open(events_file, "w") as f:
            json.dump(events, f, indent=2)

        cache[cache_key] = {"last_scraped": datetime.now().isoformat()}
        save_cache(cache)
        print(f"‚úÖ Scraped {len(events)} events for {tournament}")
        return events

    except Exception as e:
        print(f"‚ùå Error in scrape_events: {e}")
        return []


import os
import json
import re
import requests
from bs4 import BeautifulSoup

def split_boat_and_type(boat_name: str, trailing_text: str):
    """
    Cleanly split boat name and type.
    e.g. boat_name='Magic Moment', trailing_text="55' Jarrett Bay"
    """
    trailing_text = trailing_text.strip()
    if trailing_text and re.match(r"^\d{2,2}'", trailing_text):
        return boat_name, trailing_text
    return f"{boat_name} {trailing_text}".strip(), ""

def scrape_leaderboard(tournament=None, force=False):
    tournament = tournament or get_current_tournament()

    # 1Ô∏è‚É£ Load master JSON
    try:
        remote = requests.get(MASTER_JSON_URL, timeout=30).json()
    except Exception as e:
        print(f"‚ùå Failed to load master JSON: {e}")
        return []

    key = next((k for k in remote if k.lower() == tournament.lower()), None)
    if not key:
        print(f"‚ùå Tournament '{tournament}' not found in master JSON.")
        return []

    leaderboard_url = remote[key].get("leaderboard")
    if not leaderboard_url:
        print(f"‚ùå No leaderboard URL for '{tournament}'.")
        return []

    print(f"üì° Scraping leaderboard for {tournament} ‚Üí {leaderboard_url}")
    html = fetch_html(leaderboard_url)
    if not html:
        return []

    os.makedirs("debug", exist_ok=True)
    with open("debug/leaderboard_debug.html", "w", encoding="utf-8") as f:
        f.write(html)

    soup = BeautifulSoup(html, "html.parser")
    leaderboard = []

    # ‚úÖ Use dropdown category names
    categories = [a.get_text(strip=True) for a in soup.select("ul.dropdown-menu li a.leaderboard-nav")]
    print(f"Found {len(categories)} categories: {categories}")

    for category in categories:
        tab_link = soup.find("a", string=lambda x: x and x.strip() == category)
        if not tab_link:
            continue

        tab_id = tab_link.get("href")
        tab = soup.select_one(tab_id)
        if not tab:
            continue

        for row in tab.select("tr.montserrat"):
            cols = row.find_all("td")
            if len(cols) < 2:
                continue

            rank = cols[0].get_text(strip=True)
            boat_block = cols[1]
            points = cols[-1].get_text(strip=True)

            h4 = boat_block.find("h4")
            name = h4.get_text(strip=True) if h4 else ""
            text_after = boat_block.get_text(" ", strip=True).replace(name, "").strip()

            # Default assignment
            angler, boat, btype = None, name, text_after

            # If points is weight and text_after has no length ‚Üí it's an angler
            if "lb" in points.lower() and "'" not in text_after:
                angler, boat, btype = name, None, None
            else:
                boat, btype = split_boat_and_type(name, text_after)

            uid = normalize_boat_name(boat or angler or f"rank_{rank}")

            leaderboard.append({
                "rank": rank,
                "category": category,           # ‚úÖ Real dropdown category
                "angler": angler,
                "boat": boat,
                "type": btype,
                "points": points,
                "uid": uid,
                "image_path": f"/static/images/boats/{uid}.webp" if boat else ""
            })

    print(f"‚úÖ Scraped {len(leaderboard)} leaderboard entries for {tournament}")

    # Save to per-tournament cache
    lb_file = get_cache_path(tournament, "leaderboard.json")
    os.makedirs(os.path.dirname(lb_file), exist_ok=True)
    with open(lb_file, "w") as f:
        json.dump(leaderboard, f, indent=2)

    return leaderboard
    
MAX_IMG_SIZE = (400, 400)  # Max width/height
IMG_QUALITY = 70           # JPEG/WEBP quality
BOAT_FOLDER = "static/images/boats"

def optimize_boat_image(file_path):
    """Resize and compress a single boat image and save WebP version."""
    try:
        if not os.path.exists(file_path):
            return

        # Skip very small files (~already optimized)
        if os.path.getsize(file_path) < 50_000:
            return

        with Image.open(file_path) as img:
            img_format = img.format or "JPEG"

            # Resize in-place if larger than target
            if img.width > MAX_IMG_SIZE[0] or img.height > MAX_IMG_SIZE[1]:
                img.thumbnail(MAX_IMG_SIZE)

            # Overwrite original with optimized JPEG/PNG
            img.save(file_path, optimize=True, quality=IMG_QUALITY)

            # Create WebP version for faster browsers
            webp_path = os.path.splitext(file_path)[0] + ".webp"
            img.save(webp_path, format="WEBP", optimize=True, quality=IMG_QUALITY)

            print(f"‚úÖ Optimized {os.path.basename(file_path)} ({img.width}x{img.height}) -> WebP saved")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to optimize {file_path}: {e}")


def optimize_all_boat_images():
    """Optimize all boat images in static folder (startup or fallback)."""
    os.makedirs(BOAT_FOLDER, exist_ok=True)
    optimized_count = 0
    for fname in os.listdir(BOAT_FOLDER):
        if fname.lower().endswith((".jpg", ".jpeg", ".png")):
            fpath = os.path.join(BOAT_FOLDER, fname)
            optimize_boat_image(fpath)
            optimized_count += 1
    print(f"üéâ Boat image optimization complete ({optimized_count} checked)")
    
# Routes
@app.route('/')
def homepage():
    return send_from_directory('templates', 'index.html')

@app.route('/participants')
def participants_page():
    return send_from_directory('static', 'participants.html')

@app.route('/static/<path:filename>')
def serve_static(filename):
    return send_from_directory('static', filename)

@app.route('/scrape/participants')
def scrape_participants_route():
    limit = int(request.args.get('limit', 100))
    offset = int(request.args.get('offset', 0))
    participants = scrape_participants(force=True)
    sliced = participants[offset:offset + limit]
    return jsonify({
        "count": len(participants),
        "participants": sliced,
        "status": "ok"
    })

@app.route("/scrape/leaderboard")
def scrape_leaderboard_route():
    force = request.args.get("force") == "1"
    tournament = get_current_tournament()
    data = scrape_leaderboard(tournament, force=force)
    return jsonify({"status": "ok" if data else "error", "leaderboard": data})


@app.route("/participants_data")
def participants_data():
    print("üì• /participants_data requested")
    tournament = get_current_tournament()
    participants_file = get_cache_path(tournament, "participants.json")
    participants = []

    def prefer_webp(path: str) -> str:
        """Return .webp version if it exists, else original path."""
        if not path:
            return "/static/images/boats/default.jpg"
        base, ext = os.path.splitext(path)
        webp_path = base + ".webp"
        # Remove leading slash for filesystem check
        if os.path.exists(webp_path.lstrip("/")):
            return webp_path
        return path

    try:
        # 1Ô∏è‚É£ Load tournament-specific participants.json
        if os.path.exists(participants_file) and os.path.getsize(participants_file) > 0:
            with open(participants_file) as f:
                participants = json.load(f)

        # 2Ô∏è‚É£ If JSON is missing or empty, force scrape to create it
        if not participants:
            print(f"‚ö†Ô∏è No participants.json for {tournament} ‚Äî scraping immediately...")
            participants = scrape_participants(force=True)

        # 3Ô∏è‚É£ Ensure every participant has a valid image path and prefer .webp
        for p in participants:
            p["image_path"] = prefer_webp(p.get("image_path", ""))

    except Exception as e:
        print(f"‚ö†Ô∏è Error loading participants: {e}")

    # 4Ô∏è‚É£ Always sort alphabetically by boat name
    participants.sort(key=lambda p: p.get("boat", "").lower())

    return jsonify({
        "status": "ok",
        "participants": participants,
        "count": len(participants)
    })

@app.route("/scrape/events")
def get_events():
    settings = load_settings()
    tournament = get_current_tournament()
    events_file = get_cache_path(tournament, "events.json")
    participants_file = get_cache_path(tournament, "participants.json")

    # ‚úÖ Ensure participants cache exists before scraping events
    if not os.path.exists(participants_file):
        print("‚è≥ No participants yet ‚Äî scraping them first...")
        scrape_participants(force=True)
        # Wait briefly for participants to be saved
        for _ in range(10):
            if os.path.exists(participants_file):
                break
            time.sleep(0.5)

    # ‚úÖ Demo mode logic
    if settings.get("data_source") == "demo":
        data = load_demo_data(tournament)
        all_events = data.get("events", [])
        now = datetime.now()

        # ‚úÖ Filter out events that are in the future
        filtered = []
        for e in all_events:
            try:
                ts = date_parser.parse(e["timestamp"])
                if ts <= now:
                    filtered.append(e)
            except:
                continue

        # ‚úÖ Sort newest first
        filtered.sort(key=lambda e: e["timestamp"], reverse=True)

        return jsonify({
            "status": "ok",
            "count": len(filtered),
            "events": filtered[:100]
        })

    # ‚úÖ Live mode
    force = request.args.get("force", "false").lower() == "true"
    events = scrape_events(force=force, tournament=tournament)

    if not events and os.path.exists(events_file):
        with open(events_file, "r") as f:
            events = json.load(f)

    events = sorted(events, key=lambda e: e["timestamp"], reverse=True)

    return jsonify({
        "status": "ok" if events else "error",
        "count": len(events),
        "events": events[:100]
    })





@app.route("/scrape/all")
def scrape_all():
    """Force-scrape all data (events, participants, leaderboard) for the current tournament."""
    tournament = get_current_tournament()
    print(f"üîÅ Starting full scrape for tournament: {tournament}")

    # ‚úÖ Force scrape participants first (ensures images for leaderboard)
    participants = scrape_participants(force=True)

    # ‚úÖ Force scrape events
    events = scrape_events(force=True, tournament=tournament)

    # ‚úÖ Force scrape leaderboard (caches images from participants)
    leaderboard = scrape_leaderboard(tournament, force=True)

    return jsonify({
        "status": "ok",
        "tournament": tournament,
        "events": len(events),
        "participants": len(participants),
        "leaderboard": len(leaderboard),
        "message": "Scraped all data and cached it."
    })


@app.route("/status")
def get_status():
    try:
        cache = load_cache()
        tournament = get_current_tournament()
        data_source = load_settings().get("data_source", "live")
        status = {
            "mode": data_source,
            "tournament": tournament,
            "participants_last_scraped": None,
            "events_last_scraped": None,
            "participants_cache_fresh": False,
            "events_cache_fresh": False,
        }

        # Build dynamic keys
        part_key = f"{tournament}_participants"
        event_key = f"{tournament}_events"

        # Check and format timestamps
        if part_key in cache:
            ts = cache[part_key].get("last_scraped")
            status["participants_last_scraped"] = ts
            status["participants_cache_fresh"] = is_cache_fresh(cache, part_key, 1440)

        if event_key in cache:
            ts = cache[event_key].get("last_scraped")
            status["events_last_scraped"] = ts
            status["events_cache_fresh"] = is_cache_fresh(cache, event_key, 2)

        return jsonify(status)

    except Exception as e:
        print(f"‚ùå Error in /status: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

# ==========================================
# Alerts + Settings Combined Persistence
# ==========================================

@app.route('/alerts/list', methods=['GET'])
def list_alerts():
    """Return all alert subscriber emails/SMS gateways."""
    return jsonify(load_alerts())

@app.route('/alerts/subscribe', methods=['POST'])
def subscribe_alerts():
    """Add new alert subscribers to alerts.json."""
    data = request.get_json()
    new_emails = data.get('sms_emails', [])
    alerts = load_alerts()

    for email in new_emails:
        if email not in alerts:
            alerts.append(email)

    save_alerts(alerts)
    return jsonify({"status": "subscribed", "count": len(alerts)})

from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage
from email.utils import formataddr
from datetime import datetime
from PIL import Image
import os, io, smtplib

@app.route('/alerts/test', methods=['GET'])
def test_alerts():
    """Send a test email alert with Palmer Lou image inline in the email body."""
    boat_name = "Palmer Lou"
    action = "Hooked Up"
    action_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    image_path = "static/images/palmer_lou.jpg"

    recipients = load_alerts()
    if not recipients:
        return jsonify({"status": "no_subscribers"}), 404

    success = 0
    for recipient in recipients:
        try:
            # Create HTML email with inline image
            msg = MIMEMultipart("related")
            msg['From'] = formataddr(("BigRock Alerts", SMTP_USER))
            msg['To'] = recipient
            msg['Subject'] = f"{boat_name} {action} at {action_time}"

            # Create alternative plain+html body
            msg_alternative = MIMEMultipart("alternative")
            msg.attach(msg_alternative)

            # Plain text fallback
            text_body = f"üö§ {boat_name} {action}!\nTime: {action_time}\n\nBigRock Live Alert"
            msg_alternative.attach(MIMEText(text_body, "plain"))

            # HTML body referencing the inline image
            html_body = f"""
            <html>
            <body>
                <p>üö§ <b>{boat_name}</b> {action}!<br>
                Time: {action_time}</p>
                <img src="cid:boat_image" style="max-width: 600px; height: auto;">
            </body>
            </html>
            """
            msg_alternative.attach(MIMEText(html_body, "html"))

            # Attach the boat image inline
            if os.path.exists(image_path):
                try:
                    with Image.open(image_path) as img:
                        img.thumbnail((600, 600))
                        img_bytes = io.BytesIO()
                        img.save(img_bytes, format="JPEG", quality=70)
                        img_bytes.seek(0)
                        image = MIMEImage(img_bytes.read(), name=os.path.basename(image_path))
                        image.add_header("Content-ID", "<boat_image>")
                        image.add_header("Content-Disposition", "inline", filename=os.path.basename(image_path))
                        msg.attach(image)
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not resize/attach image: {e}")
            else:
                print(f"‚ùå Image not found at {image_path}")

            # Send email
            with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
                server.starttls()
                server.login(SMTP_USER, SMTP_PASS)
                server.sendmail(SMTP_USER, [recipient], msg.as_string())

            print(f"‚úÖ Test email sent to {recipient} with Palmer Lou image inline")
            success += 1

        except Exception as e:
            print(f"‚ùå Failed to send to {recipient}: {e}")

    return jsonify({"status": "sent", "success_count": success})




# ==========================================
# Settings API with Alerts Integration
# ==========================================

@app.route('/api/settings', methods=['GET', 'POST'])
def api_settings():
    """Get or update app settings, including alert recipients."""
    if request.method == 'POST':
        settings_data = request.get_json()
        if not settings_data:
            return jsonify({'status': 'error', 'message': 'Invalid JSON'}), 400

        old_settings = load_settings()
        old_tournament = old_settings.get("tournament")
        new_tournament = settings_data.get("tournament")
        new_mode = settings_data.get("data_source")

        # ‚úÖ Ensure sound fields exist
        settings_data.setdefault(
            "followed_sound",
            old_settings.get("followed_sound", "fishing reel")
        )
        settings_data.setdefault(
            "boated_sound",
            old_settings.get("boated_sound", "fishing reel")
        )

        # ‚úÖ Save SMS/email alerts
        sms_emails = settings_data.get("sms_emails", [])
        save_alerts(sms_emails)

        # ‚úÖ Save settings.json
        with open(SETTINGS_FILE, 'w') as f:
            json.dump(settings_data, f, indent=4)

        # ‚úÖ Trigger scrapes if tournament changed in live mode
        if new_tournament != old_tournament and new_mode == "live":
            print(f"üîÑ Tournament changed: {old_tournament} ‚Üí {new_tournament}")
            run_in_thread(lambda: scrape_events(force=True, tournament=new_tournament), "events")
            run_in_thread(lambda: scrape_participants(force=True), "participants")

        # ‚úÖ Handle demo mode data caching
        save_demo_data_if_needed(settings_data, old_settings)

        return jsonify({'status': 'success'})

    # ----- GET Settings -----
    settings = load_settings()
    settings.setdefault("followed_sound", "Fishing Reel")
    settings.setdefault("boated_sound", "Fishing Reel")

    # ‚úÖ Include alerts in GET response
    settings["sms_emails"] = load_alerts()

    return jsonify(settings)


@app.route('/settings-page/')
def settings_page():
    return send_from_directory('static', 'settings.html')

@app.route("/generate_demo")
def generate_demo():
    """Force regenerate demo files for events and leaderboard."""
    try:
        tournament = get_current_tournament()
        events = scrape_events(force=True, tournament=tournament)
        leaderboard = scrape_leaderboard(tournament, force=True)

        injected = inject_hooked_up_events(events, tournament)

        with open(DEMO_EVENTS_FILE, 'w') as f:
            json.dump(injected, f, indent=2)
        with open(DEMO_LEADERBOARD_FILE, 'w') as f:
            json.dump(leaderboard, f, indent=2)

        print(f"‚úÖ [DEMO] Demo files written: {len(injected)} events, {len(leaderboard)} leaderboard entries")
        return jsonify({"status": "ok", "events": len(injected)})

    except Exception as e:
        print(f"‚ùå Error generating demo data: {e}")
        return jsonify({"status": "error", "message": str(e)})

@app.route("/leaderboard")
def leaderboard_page():
    return send_from_directory('static', 'leaderboard.html')

@app.route("/api/leaderboard")
def api_leaderboard():
    tournament = get_current_tournament()
    lb_file = get_cache_path(tournament, "leaderboard.json")

    leaderboard = []
    cache_valid = False

    # 1Ô∏è‚É£ Load cached leaderboard if it exists
    if os.path.exists(lb_file) and os.path.getsize(lb_file) > 0:
        with open(lb_file) as f:
            leaderboard = json.load(f)
        # Consider cache valid if category doesn't look like weight
        if leaderboard and not any("lb" in (e.get("category") or "").lower() for e in leaderboard):
            cache_valid = True

    # 2Ô∏è‚É£ Force scrape if cache missing or invalid
    if not cache_valid:
        print("‚ö†Ô∏è Leaderboard cache invalid or empty ‚Äî scraping fresh")
        leaderboard = scrape_leaderboard(tournament, force=True)

    return jsonify({
        "status": "ok" if leaderboard else "error",
        "leaderboard": leaderboard
    })
    
@app.route("/hooked")
def get_hooked_up_events():
    settings = load_settings()
    tournament = get_current_tournament()
    data_source = settings.get("data_source", "live").lower()

    now = datetime.now()
    events = []

    # Load events from appropriate source
    if data_source == "demo":
        data = load_demo_data(tournament)
        events = data.get("events", [])
        # Only consider events that have "occurred" in demo timeline
        events = [
            e for e in events
            if date_parser.parse(e["timestamp"]) <= now
        ]
    else:
        events_file = get_cache_path(tournament, "events.json")
        if os.path.exists(events_file):
            with open(events_file, "r") as f:
                events = json.load(f)

    # Sort events chronologically
    events.sort(key=lambda e: date_parser.parse(e["timestamp"]))

    hooked_feed = []

    if data_source == "demo":
        # ‚úÖ DEMO MODE: resolve using hookup_id
        resolved_ids = set()

        # Collect all resolution event keys
        for e in events:
            if e["event"] in ["Released", "Boated"] or \
               "pulled hook" in e.get("details", "").lower() or \
               "wrong species" in e.get("details", "").lower():
                # In demo mode, use event uid+timestamp as resolution key
                key = f"{e['uid']}_{date_parser.parse(e['timestamp']).replace(microsecond=0).isoformat()}"
                resolved_ids.add(key)

        for e in events:
            if e["event"] != "Hooked Up":
                continue

            key = e.get("hookup_id")
            if not key or key not in resolved_ids:
                hooked_feed.append(e)

    else:
        # ‚úÖ LIVE MODE: resolve oldest hooked per boat
        active_hooks = {}  # boat_uid -> list of hooked events
        for e in events:
            uid = e.get("uid")
            etype = e.get("event", "").lower()

            if etype == "hooked up":
                active_hooks.setdefault(uid, []).append(e)
            elif etype in ["boated", "released"] or \
                 "pulled hook" in e.get("details", "").lower() or \
                 "wrong species" in e.get("details", "").lower():
                # Resolve oldest hooked for that boat if any
                if uid in active_hooks and active_hooks[uid]:
                    active_hooks[uid].pop(0)

        # Remaining hooked events are unresolved
        for boat_hooks in active_hooks.values():
            hooked_feed.extend(boat_hooks)

    # ‚úÖ Sort newest first for feed
    hooked_feed.sort(key=lambda e: date_parser.parse(e["timestamp"]), reverse=True)

    return jsonify({
        "status": "ok",
        "count": len(hooked_feed),
        "events": hooked_feed[:50]  # limit to 50 to keep feed clean
    })


@app.route('/bluetooth/status')
def bluetooth_status():
    return jsonify({"enabled": True})

@app.route('/bluetooth/scan')
def bluetooth_scan():
    return jsonify({"devices": [{"name": "Test Device", "mac": "00:11:22:33:44:55", "connected": False}]})

@app.route('/bluetooth/connect', methods=['POST'])
def bluetooth_connect():
    data = request.get_json()
    print(f"Connecting to: {data['mac']}")
    return jsonify({"status": "ok"})

@app.route('/wifi/scan')
def wifi_scan():
    try:
        scan_result = subprocess.check_output(['nmcli', '-t', '-f', 'SSID,SIGNAL,IN-USE', 'dev', 'wifi'], text=True)
        seen = {}
        connected_ssid = None

        for line in scan_result.strip().split('\n'):
            parts = line.strip().split(':')
            if len(parts) >= 3:
                ssid, signal, in_use = parts
                if not ssid.strip():
                    continue  # Skip empty SSIDs

                try:
                    signal = int(signal)
                except ValueError:
                    continue  # Skip invalid signal entries

                is_connected = in_use.strip() == '*'

                # Keep only the strongest signal or the connected one
                if ssid not in seen or is_connected or signal > seen[ssid]['signal']:
                    seen[ssid] = {
                        'ssid': ssid,
                        'signal': signal,
                        'connected': is_connected
                    }

                if is_connected:
                    connected_ssid = ssid

        networks = list(seen.values())
        return jsonify({'networks': networks, 'connected': connected_ssid})
    except Exception as e:
        print(f"‚ùå Wi-Fi scan error: {e}")
        return jsonify({'networks': [], 'connected': None})



@app.route('/wifi/connect', methods=['POST'])
def wifi_connect():
    data = request.get_json()
    ssid = data.get('ssid')
    password = data.get('password', '')

    if not ssid:
        return jsonify({'status': 'error', 'message': 'Missing SSID'}), 400

    try:
        print(f"üîå Attempting connection to: {ssid}")

        # Attempt to connect (without disconnecting current Wi-Fi)
        cmd = ['sudo', 'nmcli', 'dev', 'wifi', 'connect', ssid]
        if password:
            cmd += ['password', password]

        result = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)
        print(f"‚úÖ Connected: {result}")

        # Optional: delete old connections if you only want one active profile
        # subprocess.run(['sudo', 'nmcli', 'connection', 'delete', 'OldSSID'], check=False)

        return jsonify({'status': 'ok', 'message': result})

    except subprocess.CalledProcessError as e:
        # If connection fails, we do NOT disconnect the current Wi-Fi
        print(f"‚ùå nmcli error: {e.output}")
        if "Secrets were required" in e.output:
            return jsonify({
                'status': 'error',
                'message': 'Password required for new network',
                'code': 'password_required'
            }), 400
        return jsonify({'status': 'error', 'message': e.output.strip()}), 500




@app.route('/wifi/disconnect', methods=['POST'])
def wifi_disconnect():
    try:
        result = subprocess.check_output(['nmcli', '-t', '-f', 'NAME,TYPE,DEVICE', 'con', 'show', '--active'], text=True)
        lines = result.strip().split('\n')

        for line in lines:
            parts = line.strip().split(':')
            if len(parts) < 3:
                continue
            name, ctype, device = parts
            if ctype == 'wifi':
                print(f"üö´ Disconnecting Wi-Fi connection: {name}")
                subprocess.check_call(['nmcli', 'con', 'down', name])
                return jsonify({'status': 'ok', 'message': f'Disconnected from {name}'})

        # Fallback: try to disconnect wlan0 directly if no connection name found
        print("‚ö†Ô∏è No connection name found ‚Äî disconnecting wlan0 directly...")
        subprocess.check_call(['nmcli', 'device', 'disconnect', 'wlan0'])
        return jsonify({'status': 'ok', 'message': 'Disconnected wlan0'})

    except subprocess.CalledProcessError as e:
        print(f"‚ùå Wi-Fi disconnect error: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500



@app.route('/launch_keyboard', methods=['POST'])
def launch_keyboard():
    try:
        env = os.environ.copy()
        env['DISPLAY'] = ':0'
        env['XAUTHORITY'] = '/home/pi/.Xauthority'
        subprocess.Popen(['onboard'], env=env)
        return jsonify({"status": "launched"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500



@app.route('/hide_keyboard', methods=['POST'])
def hide_keyboard():
    try:
        subprocess.call(['pkill', '-f', 'onboard'])
        return jsonify({"status": "hidden"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
@app.route('/sounds')
def list_sounds():
    import os
    sound_dir = os.path.join('static', 'sounds')
    try:
        files = [f for f in os.listdir(sound_dir) if f.lower().endswith('.mp3')]
        return jsonify({'files': files})
    except Exception as e:
        return jsonify({'files': [], 'error': str(e)}), 500


@app.route('/api/version')
def api_version():
    try:
        with open("version.txt") as f:
            return jsonify({"version": f.read().strip()})
    except:
        return jsonify({"version": "Unknown"})

from collections import defaultdict

@app.route("/release-summary")
def release_summary_page():
    return send_from_directory('static', 'release-summary.html')

@app.route("/release-summary-data")
def release_summary_data():
    """Return per-day release counts for Blue, White, and Sailfish, supports demo mode."""
    try:
        tournament = get_current_tournament()
        settings = load_settings()
        demo_mode = settings.get("data_source") == "demo"

        # Load events from demo or cache
        if demo_mode:
            data = load_demo_data(tournament)
            all_events = data.get("events", [])
            now = datetime.now()
            # Filter out future events in demo mode
            events = [
                e for e in all_events
                if date_parser.parse(e["timestamp"]).time() <= now.time()
            ]
        else:
            events_file = get_cache_path(tournament, "events.json")
            if not os.path.exists(events_file):
                return jsonify({"status": "ok", "summary": []})
            with open(events_file, "r") as f:
                events = json.load(f)

        # Group by date
        from collections import defaultdict
        summary = defaultdict(lambda: {
            "blue_marlins": 0,
            "white_marlins": 0,
            "sailfish": 0,
            "total_releases": 0
        })

        for e in events:
            if e["event"].lower() != "released":
                continue

            try:
                dt = date_parser.parse(e["timestamp"])
                day = dt.strftime("%Y-%m-%d")
            except:
                continue

            details = e.get("details", "").lower()
            if "blue marlin" in details:
                summary[day]["blue_marlins"] += 1
            elif "white marlin" in details:
                summary[day]["white_marlins"] += 1
            elif "sailfish" in details:
                summary[day]["sailfish"] += 1

            summary[day]["total_releases"] += 1

        # Convert to sorted list
        result = [
            {"date": k, **v}
            for k, v in sorted(summary.items(), key=lambda x: x[0], reverse=True)
        ]

        return jsonify({"status": "ok", "demo_mode": demo_mode, "summary": result})

    except Exception as e:
        print(f"‚ùå Error generating release summary: {e}")
        return jsonify({"status": "error", "message": str(e)})



import threading

import json, os, time, threading
from datetime import datetime
from dateutil import parser as date_parser

NOTIFIED_FILE = "notified.json"
emailed_events = set()

# ==========================================
# Email Alert System (Cleaned & Reliable)
# ==========================================

NOTIFIED_FILE = "notified.json"
emailed_events = set()

def load_emailed_events():
    """Load previously emailed events to avoid duplicates."""
    if os.path.exists(NOTIFIED_FILE):
        try:
            with open(NOTIFIED_FILE, "r") as f:
                return set(json.load(f))
        except:
            return set()
    return set()

def save_emailed_events():
    """Save emailed events to disk."""
    with open(NOTIFIED_FILE, "w") as f:
        json.dump(list(emailed_events), f)

def get_followed_boats():
    """Read followed boats list from settings.json."""
    settings = load_settings()
    # Ensure they are normalized to match uids
    return [normalize_boat_name(b) for b in settings.get("followed_boats", [])]

def should_email(event):
    """Only email if event is boated or boat is followed."""
    etype = event.get("event", "").lower()
    uid = event.get("uid", "")
    
    # Always send for boated events
    if "boated" in etype:
        return True

    # Send for followed boats
    followed_boats = get_followed_boats()
    if uid in followed_boats:
        return True

    return False

def process_new_event(event):
    """Send email for any new event exactly once."""
    global emailed_events
    uid = f"{event.get('timestamp')}_{event.get('uid')}_{event.get('event')}"

    # Skip duplicates
    if uid in emailed_events:
        return
    emailed_events.add(uid)
    save_emailed_events()

    if should_email(event):
        try:
            send_boat_email_alert(event)  # ‚úÖ Use existing function
            print(f"üìß Email sent for {event['boat']} - {event['event']}")
        except Exception as e:
            print(f"‚ùå Email failed for {event['boat']}: {e}")

def background_event_emailer():
    global emailed_events
    emailed_events = load_emailed_events()
    print(f"üì° Email watcher started. Loaded {len(emailed_events)} previous notifications.")

    tournament = get_current_tournament()
    events_file = get_cache_path(tournament, "events.json")

    # ‚úÖ Preload recent events to prevent flood on startup
    try:
        if os.path.exists(events_file):
            with open(events_file) as f:
                events = json.load(f)
            events.sort(key=lambda e: e["timestamp"], reverse=True)

            # Preload the newest 50 events so they won't email immediately
            for e in events[:50]:
                uid = f"{e.get('timestamp')}_{e.get('uid')}_{e.get('event')}"
                emailed_events.add(uid)

            save_emailed_events()
            print(f"‚è© Preloaded {min(50, len(events))} events as already emailed to prevent flood")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to preload events: {e}")

    # ‚úÖ Continuous watch loop
    while True:
        try:
            settings = load_settings()
            tournament = get_current_tournament()

            # Load events from demo or live
            if settings.get("data_source") == "demo":
                events = load_demo_data(tournament).get("events", [])
                now = datetime.now().time()
                events = [e for e in events if date_parser.parse(e["timestamp"]).time() <= now]
            else:
                if not os.path.exists(events_file):
                    time.sleep(5)
                    continue
                with open(events_file) as f:
                    events = json.load(f)

            # Check only the newest 50 events
            events.sort(key=lambda e: e["timestamp"], reverse=True)
            for e in events[:50]:
                process_new_event(e)  # Will only email if not in emailed_events

        except Exception as e:
            print(f"‚ö†Ô∏è Email watcher error: {e}")

        time.sleep(30)



def save_emailed_events():
    """Save emailed events to disk."""
    with open(NOTIFIED_FILE, "w") as f:
        json.dump(list(emailed_events), f)





if __name__ == '__main__':
    print("üöÄ Optimizing boat images on startup...")
    optimize_all_boat_images()

    # ‚úÖ Start email watcher
    Thread(target=background_event_emailer, daemon=True).start()

    # ‚úÖ Start Flask
    app.run(host='0.0.0.0', port=5000, debug=True)
